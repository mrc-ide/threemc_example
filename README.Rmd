---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# threemc_example

We've noticed that when running TMB, system memory is spiking during tape optimisation and we would like to understand why. This is causing some problems for running model for large countries as it means we need to run on a machine with > 500GB of memory. This amount of memory is only used for a small period of the model fit and the rest of the fit continues. We're trying to understand

* Why does the memory spike like this?
* Can we predict the size of the memory spike from size in input data, number of parameters etc.
* Can we save out the optimised tape and then use this for multiple model fits? This could allow us to do the costly optimisation on a beefy node but then run the rest of the simulation on smaller nodes.

This repo contains an example of a smaller model fit which can be run with about 11GB of RAM but still shows the large spike during optimisation.

## Prerequisites

`readr`, `sf`, `TMB` and `threemc` packages

Install threemc from github
```r
remotes::install_github("mrc-ide/threemc")
```

## Running

Run the script from the command line via `./script.R`

or from R with working dir set to root of this repo
```r
source("threemc_fit.R")
threemc_fit()
```

## Profile

Run with profile from [memprof](https://github.com/mrc-ide/memprof)

```{r}
source("threemc_fit.R")
# load shell dataset
shell_dat <- readr::read_csv("data/shell_data.csv.gz")
# load shapefiles
areas <- sf::read_sf("data/areas.geojson")

mem <- memprof::with_monitor(threemc_fit(shell_dat, areas))
plot(mem)
```
## Full model fit

Running the model with a larger country leads to huge spikes in memory usage > 500GB.

