# Cluster Example 

The example in the README is for a small country, that can be run locally. However, 
when we look at modelling a large country, such as Ghana, we must run the model 
on a high performance cluster. The spike in memory shown above can even 
exceed 500GB. 
```{r}
# load shell dataset for Ghana
shell_dat_gha <- readr::read_csv("data/shell_data_gha.csv.gz")
```

```{r, results = "false"}
# specify directory that 
root <- "~/net/unaids-naomi/threemc-orderly/contexts_4"

# cluster config
config <- didehpc::didehpc_config(
  workdir  = root,
  cluster  = "fi--didemrchnb", 
  # need to request every core of largest cluster nodes in order to fit model
  # => at lower memory usage we could be running much more concurrent models!
  template = "32Core",        
  cores    = 32
)

# create symlink in cluster contexts dir to sourced functions
source_file <- file.path(root, "threemc_fit.R")
if (.Platform$OS.type != "windows" && !file.exists(source_file)) {
  system(paste0("ln -s threemc_fit.R ", source_file))
}

# setup context for orderly task 
ctx <- context::context_save(
  path = root,
  # functions to source in current directory
  sources = "./threemc_fit.R",
  # CRAN packages
  packages = c("dplyr", "sf", "readr", "TMB", "stats"),
  # packages from github 
  package_sources = conan::conan_sources(c(
    "github::mrc-ide/threemc", 
    "github::mrc-ide/memprof"
  ))
)
# queue above contexts
obj <- didehpc::queue_didehpc(context = ctx, config = config)
```

Running our model for Ghana, we can see how the memory spike scales up 
significantly as a result of the need for additional random effects in larger 
countries. 
```{r}
# profile memory usage
t <- obj$enqueue(
  memory_use(memprof::with_monitor(threemc_fit(shell_dat_gha, areas, mod)))
)
while (!t$status() %in% c("COMPLETE", "ERROR")) {
  Sys.sleep(15)
}

# plot result
res <- t$result()
plot(res)
```
